{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Sartorius - Cell Instance Segmentation: Baseline\n\nThe following Notebook is built as a project for the closed Kaggle competetion of the same name ([here](https://www.kaggle.com/c/sartorius-cell-instance-segmentation/overview))\n\nAll data used can be found [here](https://www.kaggle.com/c/sartorius-cell-instance-segmentation/data)\n\nThis Notebook is based on the following baseline: [Sartorius - Starter Torch Mask R-CNN](https://www.kaggle.com/julian3833/sartorius-starter-torch-mask-r-cnn-lb-0-273)","metadata":{"papermill":{"duration":0.025167,"end_time":"2022-02-26T21:25:24.453921","exception":false,"start_time":"2022-02-26T21:25:24.428754","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Imports\n\nWe begin, as all great notebooks do, with some imports.","metadata":{"papermill":{"duration":0.02233,"end_time":"2022-02-26T21:25:24.500687","exception":false,"start_time":"2022-02-26T21:25:24.478357","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Standard Library Imports\nimport os\nimport time\nimport random\nimport collections\n\n# Data Science Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n#Python Imaging Library (expansion of PIL) is the de facto image processing package for Python language.\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\n# PyTorch Imports\nimport torch\nimport torchvision #The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision. Installation. \n#torchvision: https://www.youtube.com/watch?v=CU6bTEClzlw\n#pytorch used for computer vision. It has common datasets, pretrained models, can do data transformation and more\n#Example: torchvision includes common datasets such as: torchvision.datasets.MNIST(root='.', download=True)\n#torchvision also includes common data augmentation tools:\n#Ex: torchvision.transforms as T: T.Compose([RandomResizedCrop(224), T.RandomHorizontalFlip(),...])\n#torchvision.models as M   includes many transfer learning models such as resnet\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor   #https://pytorch.org/vision/stable/models.html\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor","metadata":{"papermill":{"duration":2.617856,"end_time":"2022-02-26T21:25:27.140834","exception":false,"start_time":"2022-02-26T21:25:24.522978","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-20T01:36:04.100359Z","iopub.execute_input":"2022-03-20T01:36:04.100680Z","iopub.status.idle":"2022-03-20T01:36:04.107965Z","shell.execute_reply.started":"2022-03-20T01:36:04.100625Z","shell.execute_reply":"2022-03-20T01:36:04.107050Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Fix randomness:","metadata":{"papermill":{"duration":0.022114,"end_time":"2022-02-26T21:25:27.186056","exception":false,"start_time":"2022-02-26T21:25:27.163942","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def fix_all_seeds(seed = 42):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)  #What is happening here?\n    torch.manual_seed(seed)                   #What is happening here?\n    torch.cuda.manual_seed(seed)  \n    torch.cuda.manual_seed_all(seed)\n\nfix_all_seeds()","metadata":{"papermill":{"duration":0.032318,"end_time":"2022-02-26T21:25:27.240646","exception":false,"start_time":"2022-02-26T21:25:27.208328","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-20T01:48:31.841265Z","iopub.execute_input":"2022-03-20T01:48:31.841710Z","iopub.status.idle":"2022-03-20T01:48:31.851499Z","shell.execute_reply.started":"2022-03-20T01:48:31.841664Z","shell.execute_reply":"2022-03-20T01:48:31.850712Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Configuration","metadata":{"papermill":{"duration":0.02199,"end_time":"2022-02-26T21:25:27.286666","exception":false,"start_time":"2022-02-26T21:25:27.264676","status":"completed"},"tags":[]}},{"cell_type":"code","source":"TRAIN_CSV_PATH = '../input/sartorius-cell-instance-segmentation/train.csv'\n\nTRAIN_IMAGES_PATH = '../input/sartorius-cell-instance-segmentation/train/'\nTEST_IMAGES_PATH = '../input/sartorius-cell-instance-segmentation/test/'\n\nWIDTH = 704     #How do we come up with these numbers?\nHEIGHT = 520\n\n#Reduce the Train Set\nTEST = False\n\n# Set Device\nDEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')  ###Here we tell it to use 'cuda' GPU if avail. If it's not avail then we tell it to use cpu\n\n# Declare ResNet Mean and Standard Deviation\nRESNET_MEAN = (0.485, 0.456, 0.406)              #How do we come up with these numbers?\nRESNET_STD = (0.229, 0.224, 0.225)\n\n# Model Hyperparameters\nBATCH_SIZE = 2\nEPOCHS = 8\n\n# Optimizer Hyperparameters\nMOMENTUM = 0.9\nLEARNING_RATE = 0.001\nWEIGHT_DECAY = 0.0005\n\n# Use a StepLR Scheduler\nUSE_SCHEDULER = False    ###Start here: https://www.youtube.com/watch?v=P31hB37g4Ak\n\n# Mask Confidence Threshold\nMASK_THRESHOLD = 0.5\n\n# Normalize to ResNet Mean and Standard Deviation\nNORMALIZE = False\n\nBOX_DETECTIONS_PER_IMG = 539\nMIN_SCORE = 0.59","metadata":{"papermill":{"duration":0.07768,"end_time":"2022-02-26T21:25:27.386416","exception":false,"start_time":"2022-02-26T21:25:27.308736","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-20T01:53:34.225980Z","iopub.execute_input":"2022-03-20T01:53:34.226492Z","iopub.status.idle":"2022-03-20T01:53:34.279534Z","shell.execute_reply.started":"2022-03-20T01:53:34.226455Z","shell.execute_reply":"2022-03-20T01:53:34.278814Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Training Dataset","metadata":{"papermill":{"duration":0.022132,"end_time":"2022-02-26T21:25:27.430915","exception":false,"start_time":"2022-02-26T21:25:27.408783","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Utilities","metadata":{"papermill":{"duration":0.022032,"end_time":"2022-02-26T21:25:27.475316","exception":false,"start_time":"2022-02-26T21:25:27.453284","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"##### Transformations\n\nProvided from the baseline Notebook, as well as user \"Abishek\", these transformations are reworks of the baseline torch.transformation classes but acting on both the image and mask","metadata":{"papermill":{"duration":0.02208,"end_time":"2022-02-26T21:25:27.519805","exception":false,"start_time":"2022-02-26T21:25:27.497725","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Compose:\n    def __init__(self, transforms):           # First we init\n        self.transforms = transforms    \n    \n    def __call__(self, image, target):\n        for t in self.transforms:             #what's going on here?\n            image, target = t(image, target)\n        return image, target\n\nclass VerticalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n    \n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]        #From the second to last one to the end\n            image = image.flip(-2)\n            bbox = target['boxes']                  #here we create a bounding box\n            bbox[:, [1,3]] = height - bbox[:, [3,1]]\n            target['boxes'] = bbox\n            target['masks'] = target['masks'].flip(-2)\n        return image, target\n\nclass HorizontalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n    \n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-1)\n            bbox = target['boxes']\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target['masks'] = target['masks'].flip(-1)\n        return image, target\n\nclass Normalize:\n    def __call__(self, image, target):\n        image = F.normalize(image, mean=RESNET_MEAN, std=RESNET_STD)   #this normalize function normalizes the image!\n        return image, target\n\nclass ToTensor:\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        return image, target\n\ndef get_transform(train):\n    transforms = [ToTensor()]\n    if NORMALIZE:\n        transforms.append(Normalize())\n\n    # Data Augmentation\n    if train:                                      #We only augment the train data\n        transforms.append(HorizontalFlip(0.5))     #We use the Horizontal Flip functions\n        transforms.append(VerticalFlip(0.5))       #We use the Vertical Flip - we created these functions just above\n    return Compose(transforms)","metadata":{"papermill":{"duration":0.038001,"end_time":"2022-02-26T21:25:27.580101","exception":false,"start_time":"2022-02-26T21:25:27.5421","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-12T21:33:13.122631Z","iopub.execute_input":"2022-03-12T21:33:13.123092Z","iopub.status.idle":"2022-03-12T21:33:13.141203Z","shell.execute_reply.started":"2022-03-12T21:33:13.123043Z","shell.execute_reply":"2022-03-12T21:33:13.139925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Decoding [Run-Length Encoding](https://en.wikipedia.org/wiki/Run-length_encoding)","metadata":{"papermill":{"duration":0.022426,"end_time":"2022-02-26T21:25:27.624914","exception":false,"start_time":"2022-02-26T21:25:27.602488","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def rle_decode(mask_rle, shape, color=1):\n    \"\"\"\n    params:\n        mask_rle: run-lengthas string formatted (start length)\n        shape:    (height, width) of array to return\n\n    returns:\n        numpy array, 1 - mask, 0 - background\n    \"\"\"\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = color\n    return img.reshape(shape)","metadata":{"papermill":{"duration":0.030869,"end_time":"2022-02-26T21:25:27.677957","exception":false,"start_time":"2022-02-26T21:25:27.647088","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-12T21:33:13.142993Z","iopub.execute_input":"2022-03-12T21:33:13.143683Z","iopub.status.idle":"2022-03-12T21:33:13.156159Z","shell.execute_reply.started":"2022-03-12T21:33:13.143637Z","shell.execute_reply":"2022-03-12T21:33:13.154852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training Dataset and DataLoader","metadata":{"papermill":{"duration":0.022066,"end_time":"2022-02-26T21:25:27.722558","exception":false,"start_time":"2022-02-26T21:25:27.700492","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class CellDataset(Dataset):\n    def __init__(self, image_dir, df, transforms=None, resize=False):\n        self.transforms = transforms\n        self.image_dir = image_dir\n        self.df = df\n\n        self.should_resize = resize is not False\n        if self.should_resize:\n            self.height = int(HEIGHT * resize)\n            self.width = int(WIDTH * resize)\n        else:\n            self.height = HEIGHT\n            self.width = WIDTH\n        \n        self.image_info = collections.defaultdict(dict)\n        temp_df = self.df.groupby('id')['annotation'].agg(lambda x: list(x)).reset_index()\n        for index, row in temp_df.iterrows():\n            self.image_info[index] = {\n                'image_id'    : row['id'],\n                'image_path'  : os.path.join(self.image_dir, row['id'] + '.png'),\n                'annotations' : row['annotation']\n            }\n    \n    def get_box(self, a_mask):\n        '''Get the bounding box of a given mask'''\n        pos = np.where(a_mask)\n        xmin = np.min(pos[1])\n        xmax = np.max(pos[1])\n        ymin = np.min(pos[0])\n        ymax = np.max(pos[0])\n        return [xmin, ymin, xmax, ymax]\n\n    def __getitem__(self, idx):\n        '''Get the image and target'''\n\n        img_path = self.image_info[idx]['image_path']\n        img = Image.open(img_path).convert(\"RGB\")\n\n        if self.should_resize:\n            img = img.resize((self.width, self.height), resample=Image.BILINEAR)\n\n        info = self.image_info[idx]\n\n        n_objects = len(info['annotations'])\n        masks = np.zeros((len(info['annotations']), self.height, self.width), dtype=np.uint8)\n        boxes = []\n\n        for i, annotation in enumerate(info['annotations']):\n            a_mask = rle_decode(annotation, (HEIGHT, WIDTH))\n            a_mask = Image.fromarray(a_mask)\n\n            if self.should_resize:\n                a_mask = a_mask.resize((self.width, self.height), resample=Image.BILINEAR)\n            \n            a_mask = np.array(a_mask) > 0\n            masks[i, :, :] = a_mask\n\n            boxes.append(self.get_box(a_mask))\n\n        # dummy labels\n        labels = [1 for _ in range(n_objects)]\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        iscrowd = torch.zeros((n_objects,), dtype=torch.int64)\n\n        # Required Target for Mask R-CNN\n        target = {\n            'boxes'  : boxes,\n            'labels' : labels,\n            'masks'  : masks,\n            'image_id' : image_id,\n            'area'   : area,\n            'iscrowd': iscrowd\n        }\n\n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n        \n        return img, target\n\n    def __len__(self):\n        return len(self.image_info)","metadata":{"papermill":{"duration":0.043782,"end_time":"2022-02-26T21:25:27.788687","exception":false,"start_time":"2022-02-26T21:25:27.744905","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-12T21:33:13.169353Z","iopub.execute_input":"2022-03-12T21:33:13.169825Z","iopub.status.idle":"2022-03-12T21:33:13.196083Z","shell.execute_reply.started":"2022-03-12T21:33:13.169792Z","shell.execute_reply":"2022-03-12T21:33:13.194802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally, let's load that Data!","metadata":{"papermill":{"duration":0.022025,"end_time":"2022-02-26T21:25:27.833209","exception":false,"start_time":"2022-02-26T21:25:27.811184","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df_train = pd.read_csv(TRAIN_CSV_PATH, nrows=5000 if TEST else None)       #TRAIN_CSV_PATH variable created above\nds_train = CellDataset(TRAIN_IMAGES_PATH, df_train, resize=False, transforms=get_transform(train=True))\ndl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=lambda x: tuple(zip(*x)))","metadata":{"papermill":{"duration":0.60193,"end_time":"2022-02-26T21:25:28.457368","exception":false,"start_time":"2022-02-26T21:25:27.855438","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-12T21:33:13.200025Z","iopub.execute_input":"2022-03-12T21:33:13.200401Z","iopub.status.idle":"2022-03-12T21:33:13.613837Z","shell.execute_reply.started":"2022-03-12T21:33:13.200366Z","shell.execute_reply":"2022-03-12T21:33:13.612822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Loop","metadata":{"papermill":{"duration":0.02263,"end_time":"2022-02-26T21:25:28.502706","exception":false,"start_time":"2022-02-26T21:25:28.480076","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"#### Model","metadata":{"papermill":{"duration":0.02245,"end_time":"2022-02-26T21:25:28.54744","exception":false,"start_time":"2022-02-26T21:25:28.52499","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!mkdir -p /root/.cache/torch/hub/checkpoints/\n!cp ../input/resnetmaskrcnn/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth","metadata":{"papermill":{"duration":3.166905,"end_time":"2022-02-26T21:25:31.736884","exception":false,"start_time":"2022-02-26T21:25:28.569979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-12T21:33:13.615395Z","iopub.execute_input":"2022-03-12T21:33:13.615709Z","iopub.status.idle":"2022-03-12T21:33:15.772747Z","shell.execute_reply.started":"2022-03-12T21:33:13.615655Z","shell.execute_reply":"2022-03-12T21:33:15.771374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    NUM_CLASSES = 2\n\n    if NORMALIZE:\n        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n                                                                   box_detections_per_img = BOX_DETECTIONS_PER_IMG,\n                                                                   image_mean = RESNET_MEAN,\n                                                                   image_std = RESNET_STD)\n    else:\n        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n                                                                   box_detections_per_img = BOX_DETECTIONS_PER_IMG)\n\n    # get the number of input features for the classifier\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n\n    # replace the pre-trained head with a new one\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n\n    # now get the number of input features for the mask classifier\n    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n    hidden_layer = 256\n\n    # and replace the mask predictor with a new one\n    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, NUM_CLASSES)\n\n    return model\n\n# Get the Mask R-CNN model\n# The model does classification + bounding boxes + masks, but we only care about masks\nmodel = get_model()\nmodel.to(DEVICE)\n\nfor param in model.parameters():\n    param.requires_grad = True\n\nmodel.train();","metadata":{"papermill":{"duration":3.697889,"end_time":"2022-02-26T21:25:35.46015","exception":false,"start_time":"2022-02-26T21:25:31.762261","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-12T21:33:15.775075Z","iopub.execute_input":"2022-03-12T21:33:15.775441Z","iopub.status.idle":"2022-03-12T21:33:16.748378Z","shell.execute_reply.started":"2022-03-12T21:33:15.775393Z","shell.execute_reply":"2022-03-12T21:33:16.747456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training Loop","metadata":{"papermill":{"duration":0.022845,"end_time":"2022-02-26T21:25:35.506866","exception":false,"start_time":"2022-02-26T21:25:35.484021","status":"completed"},"tags":[]}},{"cell_type":"code","source":"params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\nn_batches = len(dl_train)\n#dl_train_small = dl_train[:10] Non-subscriptable...so use the 5-6 lines below instead\ndl_train_small = []\n\ni = 0\nfor x in enumerate(dl_train,1):   #when you run this cell, run the cell above too, b/c that's where we are creating the model\n    dl_train_small.append(x)\n    if i > 2: break\n    i += 1\n\nfor epoch in range(1, EPOCHS + 1):\n    #break                         This line breaks rights before training so the model doesn't learn anything\n    print(f\"Starting Epoch: {epoch} / {EPOCHS}\")\n\n    time_start = time.time()\n    loss_accum = 0.0\n    loss_mask_accum = 0.0\n    \n\n            \n    #for batch_idx, (images, targets) in dl_train_small:\n\n    for batch_idx, (images, targets) in enumerate(dl_train,1):   #comment out this line b/c we were enumerating twice (and use the line above with our small model)\n    #for batch_idx, (images, targets) in enumerate(dl_train, 1):\n        \n            \n            \n\n\n        # Predict\n        images = list(image.to(DEVICE) for image in images)\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        loss = sum(loss for loss in loss_dict.values())\n\n        # Backprop\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Logging\n        loss_mask = loss_dict['loss_mask'].item()\n        loss_accum += loss.item()\n        loss_mask_accum += loss_mask\n\n        if batch_idx % 50 == 0:\n            print(f\"[Batch {batch_idx:3d} / {n_batches:3d}] Loss: {loss.item():7.3f} Mask Loss: {loss_mask:7.3f}\")\n\n    if USE_SCHEDULER:\n        lr_scheduler.step()\n\n    # Train Losses\n    train_loss = loss_accum / n_batches\n    train_loss_mask = loss_mask_accum / n_batches\n\n    elapsed = time.time() - time_start\n\n    torch.save(model.state_dict(), f\"pytorch_model-e{epoch}.bin\")\n    prefix = f\"[Epoch {epoch:2d} / {EPOCHS:2d}]\"\n    print(f\"{prefix} Train mask-only loss: {train_loss_mask:7.3f}\")\n    print(f\"{prefix} Train loss: {train_loss:7.3f}. [{elapsed:.0f} secs]\")","metadata":{"papermill":{"duration":2117.294572,"end_time":"2022-02-26T22:00:52.824649","exception":false,"start_time":"2022-02-26T21:25:35.530077","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-03-12T21:33:16.75264Z","iopub.execute_input":"2022-03-12T21:33:16.752969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Analyze Predictions","metadata":{"papermill":{"duration":0.039198,"end_time":"2022-02-26T22:00:52.903605","exception":false,"start_time":"2022-02-26T22:00:52.864407","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Plots: the image, the image + the ground truth mask, the image + the predicted mask\n\ndef analyze_train_sample(model, ds_train, sample_index):\n    \n    img, targets = ds_train[sample_index]\n    plt.imshow(img.numpy().transpose((1,2,0)))\n    plt.title(\"Image\")\n    plt.show()\n    \n    masks = np.zeros((HEIGHT, WIDTH))\n    for mask in targets['masks']:\n        masks = np.logical_or(masks, mask)\n    plt.imshow(img.numpy().transpose((1,2,0)))\n    plt.imshow(masks, alpha=0.3)\n    plt.title(\"Ground truth\")\n    plt.show()\n    \n    model.eval()\n    with torch.no_grad():\n        preds = model([img.to(DEVICE)])[0]\n\n    plt.imshow(img.cpu().numpy().transpose((1,2,0)))\n    all_preds_masks = np.zeros((HEIGHT, WIDTH))\n    for mask in preds['masks'].cpu().detach().numpy():\n        all_preds_masks = np.logical_or(all_preds_masks, mask[0] > MASK_THRESHOLD)\n    plt.imshow(all_preds_masks, alpha=0.4)\n    plt.title(\"Predictions\")\n    plt.show()","metadata":{"papermill":{"duration":0.051497,"end_time":"2022-02-26T22:00:52.99517","exception":false,"start_time":"2022-02-26T22:00:52.943673","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# NOTE: Puts the model in Eval Mode\nanalyze_train_sample(model, ds_train, 20)","metadata":{"papermill":{"duration":1.29822,"end_time":"2022-02-26T22:00:54.333216","exception":false,"start_time":"2022-02-26T22:00:53.034996","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analyze_train_sample(model, ds_train, 100)","metadata":{"papermill":{"duration":1.250759,"end_time":"2022-02-26T22:00:55.633225","exception":false,"start_time":"2022-02-26T22:00:54.382466","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analyze_train_sample(model, ds_train, 2)","metadata":{"papermill":{"duration":1.12797,"end_time":"2022-02-26T22:00:56.814363","exception":false,"start_time":"2022-02-26T22:00:55.686393","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CellTestDataset(Dataset):\n    def __init__(self, image_dir, transforms=None):\n        self.transforms = transforms\n        self.image_dir = image_dir\n        self.image_ids = [f[:-4]for f in os.listdir(self.image_dir)]\n    \n    def __getitem__(self, idx):\n        image_id = self.image_ids[idx]\n        image_path = os.path.join(self.image_dir, image_id + '.png')\n        image = Image.open(image_path).convert(\"RGB\")\n\n        if self.transforms is not None:\n            image, _ = self.transforms(image=image, target=None)\n        return {'image': image, 'image_id': image_id}\n\n    def __len__(self):\n        return len(self.image_ids)","metadata":{"papermill":{"duration":0.07039,"end_time":"2022-02-26T22:00:56.946462","exception":false,"start_time":"2022-02-26T22:00:56.876072","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_test = CellTestDataset(TEST_IMAGES_PATH, transforms=get_transform(train=False))","metadata":{"papermill":{"duration":0.071743,"end_time":"2022-02-26T22:00:57.078031","exception":false,"start_time":"2022-02-26T22:00:57.006288","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rle_encoding(x):\n    dots = np.where(x.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return ' '.join(map(str, run_lengths))\n\n\ndef remove_overlapping_pixels(mask, other_masks):\n    for other_mask in other_masks:\n        if np.sum(np.logical_and(mask, other_mask)) > 0:\n            mask[np.logical_and(mask, other_mask)] = 0\n    return mask","metadata":{"papermill":{"duration":0.069914,"end_time":"2022-02-26T22:00:57.207973","exception":false,"start_time":"2022-02-26T22:00:57.138059","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval();\n\nsubmission = []\nfor sample in ds_test:\n    img = sample['image']\n    image_id = sample['image_id']\n    with torch.no_grad():\n        result = model([img.to(DEVICE)])[0]\n    \n    previous_masks = []\n    for i, mask in enumerate(result[\"masks\"]):\n        \n        # Filter-out low-scoring results. Not tried yet.\n        score = result[\"scores\"][i].cpu().item()\n        if score < MIN_SCORE:\n            continue\n        \n        mask = mask.cpu().numpy()\n        # Keep only highly likely pixels\n        binary_mask = mask > MASK_THRESHOLD\n        binary_mask = remove_overlapping_pixels(binary_mask, previous_masks)\n        previous_masks.append(binary_mask)\n        rle = rle_encoding(binary_mask)\n        submission.append((image_id, rle))\n    \n    # Add empty prediction if no RLE was generated for this image\n    all_images_ids = [image_id for image_id, rle in submission]\n    if image_id not in all_images_ids:\n        submission.append((image_id, \"\"))\n\ndf_sub = pd.DataFrame(submission, columns=['id', 'predicted'])\ndf_sub.to_csv(\"submission.csv\", index=False)\ndf_sub.head()","metadata":{"papermill":{"duration":3.138018,"end_time":"2022-02-26T22:01:00.406057","exception":false,"start_time":"2022-02-26T22:00:57.268039","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}